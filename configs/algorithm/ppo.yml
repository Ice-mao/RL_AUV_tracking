# Algorithm specific parameters for PPO
# Can be merged with a main environment config

training:
  device: "cuda"
  seed: 46
  nb_envs: 2
  timesteps: 1000000
  batch_size: 64
  save_freq: 50000
  log_freq: 1000
  log_dir: "log"
  resume_path: "log/AUVTracking_v0/PPO/07-25_12/rl_model_1000000_steps.zip" # for keep training and eval
  net_arch:
    pi: [256, 256, 256]
    vf: [256, 256]

policy:
  net_arch:
    pi: [256, 256, 256]
    vf: [256, 256]
  # features_extractor:
  #   class_name: "Encoder"
  #   module_path: "auv_track_launcher.networks.student_network"
  #   kwargs:
  #     features_dim: 512
  #     num_images: 5
  #     resnet_output_dim: 128

policy_hparams:
  policy: "PPO"
  lr: 0.0003
  gamma: 0.99
  buffer_size: 100000

  # PPO specific
  n_steps: 1024
  vf_coef: 0.25
  ent_coef: 0.0
  gae_lambda: 0.95
  max_grad_norm: 0.5
  eps_clip: 0.2
  value_clip: 0.1
  norm_adv: false
