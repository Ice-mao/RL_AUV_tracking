# Algorithm specific parameters for PPO
# Can be merged with a main environment config

training:
  device: "cuda"
  seed: 46
  nb_envs: 1
  timesteps: 1000000
  batch_size: 32
  save_freq: 50000
  log_freq: 1000
  log_dir: "log"
  resume_path: "log/AUVTracking_v0/PPO/07-12_22/best_model.zip" # for keep training and eval
  net_arch:
    pi: [256, 256, 256]
    vf: [256, 256]

policy:
  net_arch:
    pi: [256, 256, 256]
    vf: [256, 256]
  features_extractor:
    class_name: "Encoder"
    module_path: "auv_track_launcher.networks.feature_network"
    kwargs:
      features_dim: 512
      resnet_output_dim: 128

  # obs_encoder:
    # _target_: diffusion_policy.model.vision.multi_image_obs_encoder.MultiImageObsEncoder
    # shape_meta: ${shape_meta}
    # rgb_model:
    #   _target_: diffusion_policy.model.vision.model_getter.get_resnet
    #   name: resnet18
    #   weights: null
    # resize_shape: null
    # crop_shape: [76, 76]
    # # constant center crop
    # random_crop: True
    # use_group_norm: True
    # share_rgb_model: False
    # imagenet_norm: True

policy_hparams:
  policy: "PPO"
  lr: 0.0003
  gamma: 0.99
  buffer_size: 100000

  # PPO specific
  n_steps: 1024
  vf_coef: 0.25
  ent_coef: 0.0
  gae_lambda: 0.95
  max_grad_norm: 0.5
  eps_clip: 0.2
  value_clip: 0.1
  norm_adv: false
