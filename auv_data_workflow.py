"""
AUVæ•°æ®é‡‡é›†å’Œè®­ç»ƒçš„å®Œæ•´æµç¨‹ç¤ºä¾‹

å±•ç¤ºä»æ•°æ®é‡‡é›†åˆ°æ¨¡å‹è®­ç»ƒçš„å®Œæ•´å·¥ä½œæµç¨‹
"""

import numpy as np
import torch
from pathlib import Path
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from auv_track_launcher.dataset.data_collector import AUVDataCollector
from auv_track_launcher.dataset.auv_tracking_dataset import AUVTrackingDataset


class AUVDataWorkflow:
    """
    AUVæ•°æ®é‡‡é›†å’Œè®­ç»ƒçš„å®Œæ•´å·¥ä½œæµç¨‹
    """
    
    def __init__(self, data_dir: str = "./auv_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®é‡‡é›†å™¨
        self.collector = None
        
        # è®­ç»ƒæ•°æ®é›†
        self.train_dataset = None
        self.val_dataset = None
        self.normalizer = None

    def phase1_collect_data(self, n_episodes: int = 100):
        """
        é˜¶æ®µ1: æ•°æ®é‡‡é›†
        
        åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½¿ç”¨ReplayBufferå®æ—¶æ”¶é›†AUVä»¿çœŸæˆ–å®éªŒæ•°æ®
        """
        print("=== é˜¶æ®µ1: æ•°æ®é‡‡é›† ===")
        
        # åˆ›å»ºæ•°æ®é‡‡é›†å™¨
        self.collector = AUVDataCollector(
            save_dir=str(self.data_dir / "collected"),
            auto_save_episodes=10
        )
        
        # æ¨¡æ‹Ÿæ•°æ®é‡‡é›†è¿‡ç¨‹
        print(f"å¼€å§‹æ”¶é›† {n_episodes} ä¸ªepisodes...")
        
        for episode_idx in range(n_episodes):
            if episode_idx % 10 == 0:
                print(f"è¿›åº¦: {episode_idx}/{n_episodes}")
            
            # å¼€å§‹æ–°episode
            self.collector.start_new_episode()
            
            # æ¨¡æ‹Ÿepisodeæ•°æ®æ”¶é›†
            episode_length = np.random.randint(200, 500)
            
            for step in range(episode_length):
                # æ¨¡æ‹Ÿè§‚æµ‹æ•°æ®ï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›æ¥è‡ªç¯å¢ƒï¼‰
                obs = self._generate_mock_observation()
                
                # æ¨¡æ‹Ÿæ§åˆ¶åŠ¨ä½œï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œè¿™äº›æ¥è‡ªæ§åˆ¶å™¨ï¼‰
                action = self._generate_mock_action()
                
                # æ¨¡æ‹Ÿå¥–åŠ±å’Œç»“æŸæ¡ä»¶
                reward = np.random.random()
                done = (step == episode_length - 1)
                
                # æ”¶é›†æ•°æ®åˆ°ReplayBuffer
                self.collector.collect_step_data(obs, action, reward, done)
        
        # ä¿å­˜æ”¶é›†çš„æ•°æ®
        data_file = self.data_dir / "auv_training_data.zarr"
        self.collector.save_to_file(str(data_file))
        
        # è¾“å‡ºé‡‡é›†ç»Ÿè®¡
        stats = self.collector.get_stats()
        print("\næ•°æ®é‡‡é›†å®Œæˆ:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        return str(data_file)

    def phase2_create_training_dataset(self, data_file: str):
        """
        é˜¶æ®µ2: åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        
        ä»ä¿å­˜çš„æ•°æ®æ–‡ä»¶åˆ›å»ºPyTorchè®­ç»ƒæ•°æ®é›†
        """
        print("\n=== é˜¶æ®µ2: åˆ›å»ºè®­ç»ƒæ•°æ®é›† ===")
        
        # é…ç½®æ•°æ®é›†å‚æ•°
        dataset_config = {
            'data_path': data_file,
            'horizon': 8,           # é¢„æµ‹åºåˆ—é•¿åº¦
            'pad_before': 2,        # å†å²è§‚æµ‹
            'pad_after': 0,         # æœªæ¥è§‚æµ‹
            'val_ratio': 0.15,      # éªŒè¯é›†æ¯”ä¾‹
            'image_size': (224, 224),
            'use_image': True,
            'use_sonar': True,
            'use_imu': True,
        }
        
        try:
            # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
            self.train_dataset = AUVTrackingDataset(**dataset_config)
            self.val_dataset = self.train_dataset.get_validation_dataset()
            
            # è·å–æ•°æ®æ ‡å‡†åŒ–å™¨
            self.normalizer = self.train_dataset.get_normalizer()
            
            # è¾“å‡ºæ•°æ®é›†ä¿¡æ¯
            stats = self.train_dataset.get_dataset_stats()
            print("è®­ç»ƒæ•°æ®é›†åˆ›å»ºæˆåŠŸ:")
            for key, value in stats.items():
                print(f"  {key}: {value}")
            
            print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_dataset)}")
            print(f"éªŒè¯é›†å¤§å°: {len(self.val_dataset)}")
            
            # æµ‹è¯•æ•°æ®åŠ è½½
            if len(self.train_dataset) > 0:
                sample = self.train_dataset[0]
                print("\næ ·æœ¬æ•°æ®ç»“æ„:")
                print("è§‚æµ‹æ•°æ®:")
                for key, value in sample['obs'].items():
                    print(f"  {key}: {value.shape}")
                print(f"åŠ¨ä½œæ•°æ®: {sample['action'].shape}")
            
            return True
            
        except Exception as e:
            print(f"æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
            return False

    def phase3_prepare_training(self):
        """
        é˜¶æ®µ3: å‡†å¤‡è®­ç»ƒ
        
        åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼Œä¿å­˜é…ç½®ç­‰
        """
        print("\n=== é˜¶æ®µ3: å‡†å¤‡è®­ç»ƒ ===")
        
        if self.train_dataset is None:
            print("é”™è¯¯: è®­ç»ƒæ•°æ®é›†æœªåˆ›å»º")
            return False
        
        # åˆ›å»ºPyTorchæ•°æ®åŠ è½½å™¨
        from torch.utils.data import DataLoader
        
        train_loader = DataLoader(
            self.train_dataset,
            batch_size=16,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            self.val_dataset,
            batch_size=16,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        print(f"è®­ç»ƒåŠ è½½å™¨: {len(train_loader)} batches")
        print(f"éªŒè¯åŠ è½½å™¨: {len(val_loader)} batches")
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        normalizer_path = self.data_dir / "normalizer.pkl"
        import pickle
        with open(normalizer_path, 'wb') as f:
            pickle.dump(self.normalizer, f)
        print(f"æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {normalizer_path}")
        
        # æµ‹è¯•ä¸€ä¸ªbatch
        try:
            batch = next(iter(train_loader))
            print(f"\næµ‹è¯•batchåŠ è½½:")
            print(f"è§‚æµ‹æ•°æ®:")
            for key, value in batch['obs'].items():
                print(f"  {key}: {value.shape}")
            print(f"åŠ¨ä½œæ•°æ®: {batch['action'].shape}")
            
        except Exception as e:
            print(f"BatchåŠ è½½æµ‹è¯•å¤±è´¥: {e}")
            return False
        
        return train_loader, val_loader

    def _generate_mock_observation(self):
        """ç”Ÿæˆæ¨¡æ‹Ÿè§‚æµ‹æ•°æ®"""
        return {
            'camera_image': np.random.randint(0, 256, (224, 224, 3), dtype=np.uint8),
            'sonar_data': np.random.uniform(0, 50, 360).astype(np.float32),
            'auv_state': np.random.randn(12).astype(np.float32),
            'target_state': np.random.randn(5).astype(np.float32)
        }

    def _generate_mock_action(self):
        """ç”Ÿæˆæ¨¡æ‹ŸåŠ¨ä½œæ•°æ®"""
        return np.random.uniform(-2, 2, 6).astype(np.float32)

    def run_complete_workflow(self, n_episodes: int = 50):
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å·¥ä½œæµç¨‹
        """
        print("å¼€å§‹AUVæ•°æ®é‡‡é›†å’Œè®­ç»ƒå‡†å¤‡å·¥ä½œæµç¨‹")
        print("=" * 50)
        
        # é˜¶æ®µ1: æ•°æ®é‡‡é›†
        data_file = self.phase1_collect_data(n_episodes)
        
        # é˜¶æ®µ2: åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        if not self.phase2_create_training_dataset(data_file):
            print("å·¥ä½œæµç¨‹å¤±è´¥: æ•°æ®é›†åˆ›å»ºé”™è¯¯")
            return False
        
        # é˜¶æ®µ3: å‡†å¤‡è®­ç»ƒ
        result = self.phase3_prepare_training()
        if result is False:
            print("å·¥ä½œæµç¨‹å¤±è´¥: è®­ç»ƒå‡†å¤‡é”™è¯¯")
            return False
        
        train_loader, val_loader = result
        
        print("\n" + "=" * 50)
        print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ!")
        print("\nç°åœ¨å¯ä»¥å¼€å§‹æ¨¡å‹è®­ç»ƒ:")
        print("1. ä½¿ç”¨ train_loader å’Œ val_loader è¿›è¡Œè®­ç»ƒ")
        print("2. ä½¿ç”¨ self.normalizer è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–")
        print("3. è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ç”¨äºAUVæ§åˆ¶")
        
        return {
            'train_loader': train_loader,
            'val_loader': val_loader,
            'normalizer': self.normalizer,
            'data_file': data_file
        }


def demonstrate_data_collection_vs_training():
    """
    æ¼”ç¤ºæ•°æ®é‡‡é›†å’Œè®­ç»ƒçš„åŒºåˆ«
    """
    print("=" * 60)
    print("AUVæ•°æ®é‡‡é›† vs è®­ç»ƒæ•°æ®é›† - æ¦‚å¿µè¯´æ˜")
    print("=" * 60)
    
    print("""
ğŸ“Š æ•°æ®é‡‡é›†é˜¶æ®µ (ä½¿ç”¨ReplayBuffer):
   ç›®çš„: åœ¨ä»¿çœŸ/å®éªŒä¸­å®æ—¶æ”¶é›†æ•°æ®
   å·¥å…·: AUVDataCollector + ReplayBuffer
   è¾“å‡º: .zarr æˆ– .npz æ–‡ä»¶
   ç‰¹ç‚¹: 
   - å®æ—¶å¢é‡å¼æ”¶é›†
   - æŒ‰episodeç»„ç»‡æ•°æ®
   - æ”¯æŒå¤§è§„æ¨¡æ•°æ®å­˜å‚¨
   - å¯ä»¥ä¸­æ–­å’Œæ¢å¤é‡‡é›†

ğŸ¯ è®­ç»ƒé˜¶æ®µ (ä½¿ç”¨Dataset):
   ç›®çš„: ä¸ºç¥ç»ç½‘ç»œè®­ç»ƒæä¾›æ•°æ®
   å·¥å…·: AUVTrackingDataset (ç»§æ‰¿PyTorch Dataset)
   è¾“å…¥: .zarr æˆ– .npz æ–‡ä»¶
   ç‰¹ç‚¹:
   - æŒ‰åºåˆ—é‡‡æ ·æ•°æ®
   - æ•°æ®é¢„å¤„ç†å’Œæ ‡å‡†åŒ–
   - æ”¯æŒæ‰¹é‡åŠ è½½
   - è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†

ğŸ”„ å®Œæ•´æµç¨‹:
   ä»¿çœŸç¯å¢ƒ â†’ ReplayBuffer â†’ æ•°æ®æ–‡ä»¶ â†’ Dataset â†’ DataLoader â†’ ç¥ç»ç½‘ç»œ
   """)


if __name__ == "__main__":
    # æ¼”ç¤ºæ¦‚å¿µ
    demonstrate_data_collection_vs_training()
    
    # è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹
    workflow = AUVDataWorkflow(data_dir="./demo_auv_data")
    result = workflow.run_complete_workflow(n_episodes=20)
    
    if result:
        print(f"\nğŸ‰ æ¼”ç¤ºå®Œæˆ! æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒAUVæ§åˆ¶ç­–ç•¥ã€‚")
